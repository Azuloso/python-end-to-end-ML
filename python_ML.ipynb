{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution:\n",
    "\n",
    "Data Source: http://archive.ics.uci.edu/ml/datasets/Bank+Marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Markup Language Editing\n",
    "\n",
    "Source: https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment = dataset + code + hyperparameters + compute environment + results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Open Source Projects of Interest </b>\n",
    "\n",
    "- <b> Databricks Mlflow </b>\n",
    "\n",
    "Source: https://mlflow.org/\n",
    "\n",
    "- <b> Airbnb's Knowledge Repository </b>\n",
    "\n",
    "As an organization grows, how do we make sure that an insight uncovered by one person effectively transfers beyond the targeted recipient? Internally, we call this scaling knowledge.\n",
    "\n",
    "Source: https://airbnb.io/projects/knowledge-repo/\n",
    "\n",
    "- <b> Uber's Manifold </b>\n",
    "\n",
    "Source: https://eng.uber.com/manifold/\n",
    "\n",
    "- <b> Uber's casualML Uplift Modelling Package </b>\n",
    "\n",
    "Source: https://github.com/uber/causalml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper as hlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust statistics\n",
    "\n",
    "from scipy import stats\n",
    "from scipy import stats as sp\n",
    "\n",
    "# test for collinearity\n",
    "\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "\n",
    "# import featuretools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# shuffle dataframe dataset\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model building\n",
    "\n",
    "# import library for naive bayes\n",
    "import sklearn.naive_bayes as nb\n",
    "\n",
    "# import library for logistic regression\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "\n",
    "# import library for decision tree\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# import library for random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn import cluster as c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split, StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, roc_curve, precision_recall_curve, auc, make_scorer, precision_score,recall_score, accuracy_score, f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracted Terminology in Machine Learning\n",
    "\n",
    "# Estimators\n",
    "# Optimisers\n",
    "# Learners\n",
    "# Classifiers\n",
    "# Regressors\n",
    "# Predictors\n",
    "# Scalers\n",
    "# Transformers\n",
    "# Normalizers\n",
    "# Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "\n",
    "r_filename = 'bank_contacts.csv'\n",
    "\n",
    "# read the data\n",
    "\n",
    "data = pd.read_csv(r_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = data['credit_application']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> Part 1 </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the process of exploring and understanding your data to find patterns, relationships, or anomalies to inform subsequent analysis.\n",
    "\n",
    "Source: https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41188, 60)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Separate Variable Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is taken from an Udemy online course Deployment of Machine Learning Models\n",
    "\n",
    "class FeaturePreparer:\n",
    "    '''\n",
    "        when we call the Feature preparer for the first time\n",
    "        we initialise it with the training data set, which is \n",
    "        then stored as an attribute(raw_data)\n",
    "    '''\n",
    "    def __init__(self, raw_data: pd.DataFrame):\n",
    "        self.raw_data = raw_data\n",
    "        self.prepared_data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.continuous = None\n",
    "        self.categorical = None\n",
    "        self.discrete = None\n",
    "        self.encoding_dict = {}\n",
    "\n",
    "    def separate_variable_types(self) -> None:\n",
    "        '''\n",
    "            find categorical variables\n",
    "        '''\n",
    "        self.categorical = [\n",
    "            var for var in self.raw_data.columns\n",
    "            if self.raw_data[var].dtype == 'O'\n",
    "        ]\n",
    "        print('There are {} categorical variables'.format(len(self.categorical)))\n",
    "\n",
    "        # find numerical variables\n",
    "        # this should be done based off training data\n",
    "        numerical = [var for var in self.raw_data.columns\n",
    "                     if self.raw_data[var].dtype != 'O']\n",
    "        print('There are {} numerical variables'.format(len(numerical)))\n",
    "\n",
    "        # find discrete variables\n",
    "        # this should be done based off training data, i.e., self.raw_data\n",
    "        self.discrete = []\n",
    "        for var in numerical:\n",
    "            if len(self.raw_data[var].unique()) < 20:\n",
    "                self.discrete.append(var)\n",
    "\n",
    "        print('There are {} discrete variables'.format(len(self.discrete)))\n",
    "\n",
    "        self.continuous = [\n",
    "            var for var in numerical if\n",
    "            var not in self.discrete and var not in ['Id', 'SalePrice']\n",
    "        ]\n",
    "        \n",
    "    def split_data(self, *, training: bool = False) -> None:\n",
    "        # if we are training for the first time, training = True, \n",
    "        # then divide into train and test\n",
    "        if training:\n",
    "            return\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.prepared_data, self.prepared_data.SalePrice, test_size=0.2, random_state=0)\n",
    "\n",
    "        print(self.X_train.shape, self.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing data\n",
    "# rare imputation\n",
    "# encode categorical variables\n",
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 categorical variables\n",
      "There are 60 numerical variables\n",
      "There are 54 discrete variables\n"
     ]
    }
   ],
   "source": [
    "C = FeaturePreparer(data)\n",
    "\n",
    "C.separate_variable_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Data Visualisation\n",
    "\n",
    "Source: https://stackabuse.com/seaborn-library-for-data-visualization-in-python-part-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f2d03d4f60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output bar chart\n",
    "\n",
    "sns.countplot(x='credit_application', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_age             0\n",
       "n_duration        0\n",
       "n_pdays           0\n",
       "n_previous        0\n",
       "n_emp_var_rate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count all NaN in a DataFrame (both columns & Rows)\n",
    "data.isnull().sum().sum()\n",
    "\n",
    "# Count total NaN at each column in DataFrame\n",
    "data.isnull().sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is taken from Udemy online course Deployment of Machine Learning Models\n",
    "\n",
    "def handle_missing_values(self):\n",
    "    \n",
    "    '''\n",
    "        fills NA in all required variables\n",
    "    '''\n",
    "    \n",
    "    for col in self.continuous:\n",
    "        if self.prepared_data.loc[:, (col)].isnull().mean() > 0:\n",
    "            # get the mean value from the training data, i.e., self.raw_data\n",
    "            mean_val = self.raw_data.loc[:, (col)].mean()\n",
    "            # replace it in the data to be passed to the model, i.e., self.prepared_data\n",
    "            self.prepared_data[col].fillna(mean_val, inplace=True)\n",
    "\n",
    "    # add label indicating 'Missing' to categorical variables\n",
    "    for var in self.categorical:\n",
    "        # replace NA in all categorical variables to be passed to the model\n",
    "        self.prepared_data[var].fillna('Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Imbalanced Datasets\n",
    "\n",
    "\n",
    "An imbalanced dataset is one where the number of observations belonging to one group or class is significantly higher than those belonging to the other classes.\n",
    "\n",
    "Machine Learning algorithms are very likely to produce faulty classifiers when they are trained with imbalanced datasets. These algorithms tend to show a bias for the majority class, treating the minority class as a noise in the dataset. With many standard classifier algorithms, such as Logistic Regression, Naive Bayes and Decision Trees, there is a likelihood of the wrong classification of the minority class.\n",
    "\n",
    "#### Methods of Handling Imbalanced Datasets\n",
    "\n",
    "Oversampling: This method involves reducing or eliminating the imbalance in the dataset by replicating or creating new observations of the minority class. \n",
    "\n",
    "#### Oversampling Techniques:\n",
    "\n",
    "1. Random Oversampling\n",
    "2. Cluster-Based Oversampling\n",
    "3. Synthetic Oversampling\n",
    "4. Modified Synthetic Oversampling\n",
    "\n",
    "#### Undersampling Techniques:\n",
    "\n",
    "1. Random Undersampling\n",
    "\n",
    "Source: https://medium.com/coinmonks/handling-imbalanced-datasets-predicting-credit-card-fraud-544f5e74e0fd\n",
    "\n",
    "Source: https://www.datacamp.com/community/tutorials/diving-deep-imbalanced-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Undersampling\n",
    "# sample size is manually chosen by the user\n",
    "\n",
    "# shuffle the dataset and select n observations\n",
    "shuffled_df = data.sample(n=40000,random_state=4, replace=True)\n",
    "\n",
    "# Put all the credit_application in a separate dataset.\n",
    "c_app_df = shuffled_df.loc[shuffled_df['credit_application'] == 1]\n",
    "\n",
    "# Randomly select n observations from the non-fraud (majority class)\n",
    "c_non_app_df = shuffled_df.loc[shuffled_df['credit_application'] == 0].sample(n=4400,random_state=42, replace=True)\n",
    "\n",
    "# Concatenate both dataframes again\n",
    "balanced_data = pd.concat([c_app_df, c_non_app_df])\n",
    "\n",
    "# plot the dataset after the undersampling\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot('credit_application', data=balanced_data)\n",
    "plt.title('Balanced Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8929, 60)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Check Collinearity of Features\n",
    "\n",
    "- Multicollinearity: Occurs when independent variables in a regression model are correlated.\n",
    "- Structural Multicollinearity:\n",
    "- Data Multicollinearity:\n",
    "\n",
    "<b> Methods to Find Collinearity </b>\n",
    "\n",
    "- Principal Component Analysis (PAC): \n",
    "- Using Variance Inflation Factor (VIF): We can determine if two independent variables are collinear with each other. \n",
    "- Analysis of Variance (ANOVA): \n",
    "\n",
    "\n",
    "<b> Notes: </b>\n",
    "\n",
    "Multicollinearity makes it difficult to ascertain how important a feature is to the target variable.\n",
    "\n",
    "Source: https://towardsdatascience.com/multicollinearity-in-data-science-c5f6c0fe6edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['credit_application'], axis=1)\n",
    "y = data['credit_application']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceVIF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, thresh=10.0, impute=True, impute_strategy='median'):\n",
    "        # From looking at documentation, values between 5 and 10 are \"okay\".\n",
    "        # Above 10 is too high and so should be removed.\n",
    "        self.thresh = thresh\n",
    "        \n",
    "        # The statsmodel function will fail with NaN values, as such we have to impute them.\n",
    "        # By default we impute using the median value.\n",
    "        # This imputation could be taken out and added as part of an sklearn Pipeline.\n",
    "        if impute:\n",
    "            self.imputer = Imputer(strategy=impute_strategy)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print('ReduceVIF fit')\n",
    "        if hasattr(self, 'Imputer'):\n",
    "            self.Imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('ReduceVIF transform')\n",
    "        columns = X.columns.tolist()\n",
    "        if hasattr(self, 'Imputer'):\n",
    "            X = pd.DataFrame(self.Imputer.transform(X), columns=columns)\n",
    "        return ReduceVIF.calculate_vif(X, self.thresh)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_vif(X, thresh=5.0):\n",
    "        # Taken from https://stats.stackexchange.com/a/253620/53565 and modified\n",
    "        dropped=True\n",
    "        while dropped:\n",
    "            variables = X.columns\n",
    "            dropped = False\n",
    "            vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n",
    "            \n",
    "            max_vif = max(vif)\n",
    "            if max_vif > thresh:\n",
    "                maxloc = vif.index(max_vif)\n",
    "                print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n",
    "                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "                dropped=True\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balanced_X = balanced_data.drop(['credit_application'], axis=1)\n",
    "y = balanced_data['credit_application']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReduceVIF fit\n",
      "ReduceVIF transform\n",
      "Dropping job_admin with vif=inf\n",
      "Dropping marital_divorced with vif=inf\n",
      "Dropping edu_basic_4y with vif=inf\n",
      "Dropping housing_unknown with vif=inf\n",
      "Dropping contact_cellular with vif=inf\n",
      "Dropping prev_ctc_outcome_failure with vif=inf\n",
      "Dropping month_apr with vif=inf\n",
      "Dropping n_nr_employed with vif=170.9065365312705\n",
      "Dropping n_emp_var_rate with vif=90.42046974350791\n",
      "Dropping dow_thu with vif=48.941561252071565\n",
      "Dropping n_pdays with vif=41.16610915451281\n",
      "Dropping prev_ctc_outcome_nonexistent with vif=15.064931009963574\n",
      "Dropping n_cons_price_idx with vif=11.978598814409073\n"
     ]
    }
   ],
   "source": [
    "transformer = ReduceVIF()\n",
    "\n",
    "# Iterates through specified columns in the dataset\n",
    "# Only use n columns for speed in this example\n",
    "X = transformer.fit_transform(Balanced_X[Balanced_X.columns[-60:]], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Find Empirical and Fit Theoretical Distributions to Features\n",
    "\n",
    "- Normal Distribution: This is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n",
    "\n",
    "Source: https://docs.pymc.io/Probability_Distributions.html\n",
    "\n",
    "Source: https://medium.com/@nsethi610/data-cleaning-scale-and-normalize-data-4a7c781dd628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(data['n_age'], kde=True, rug=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 82 distribution python packages\n",
    "\n",
    "sub_dist = ['uniform', 'norm', 'gamma', 'lognorm', 'chi2']\n",
    "dist_names = [ 'alpha', 'anglit', 'arcsine', 'beta', 'betaprime', 'bradford', \n",
    "              'burr', 'cauchy', 'chi', 'chi2', 'cosine', 'dgamma', 'dweibull', \n",
    "              'erlang', 'expon', 'exponweib', 'exponpow', 'f', 'fatiguelife', 'fisk', \n",
    "              'foldcauchy', 'foldnorm', 'frechet_r', 'frechet_l', 'genlogistic', 'genpareto', \n",
    "              'genexpon', 'genextreme', 'gausshyper', 'gamma', 'gengamma', 'genhalflogistic', 'gilbrat', \n",
    "              'gompertz', 'gumbel_r', 'gumbel_l', 'halfcauchy', 'halflogistic', 'halfnorm', 'hypsecant', \n",
    "              'invgamma', 'invgauss', 'invweibull', 'johnsonsb', 'johnsonsu', 'ksone', 'kstwobign', 'laplace', \n",
    "              'logistic', 'loggamma', 'loglaplace', 'lognorm', 'lomax', 'maxwell', 'mielke', 'nakagami', 'ncx2', \n",
    "              'ncf', 'nct', 'norm', 'pareto', 'pearson3', 'powerlaw', 'powerlognorm', 'powernorm', 'rdist', 'reciprocal', \n",
    "              'rayleigh', 'rice', 'recipinvgauss', 'semicircular', 't', 'triang', 'truncexpon', 'truncnorm', \n",
    "              'tukeylambda', 'uniform', 'vonmises', 'wald', 'weibull_min', 'weibull_max', 'wrapcauchy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the python distribution packages to a python list\n",
    "\n",
    "dist_continu = [d for d in dir(stats) if\n",
    "                 isinstance(getattr(stats, d), stats.rv_continuous)]\n",
    "dist_discrete = [d for d in dir(stats) if\n",
    "                  isinstance(getattr(stats, d), stats.rv_discrete)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Class with methods to fit python distributions to data\n",
    "\n",
    "class Distribution(object):\n",
    "    \n",
    "    def __init__(self,dist_names_list = []):\n",
    "        self.dist_names_c = dist_continu\n",
    "        self.dist_names_d = dist_discrete\n",
    "        self.dist_results = []\n",
    "        self.params = {}\n",
    "        \n",
    "        self.DistributionName = \"\"\n",
    "        self.PValue = 0\n",
    "        self.Param = None\n",
    "        \n",
    "        self.isFitted = False\n",
    "        \n",
    "    # for continous variables\n",
    "    # finds the best fitted distribution\n",
    "    \n",
    "    def Fit_C(self, y):\n",
    "        self.dist_results = []\n",
    "        self.params = {}\n",
    "        for dist_name in self.dist_names_c:\n",
    "            dist = getattr(sp, dist_name)\n",
    "            param = dist.fit(y)\n",
    "\n",
    "            self.params[dist_name] = param\n",
    "            #Applying the Kolmogorov-Smirnov test\n",
    "            D, p = sp.stats.kstest(y, dist_name, args=param)\n",
    "            self.dist_results.append((dist_name,p))\n",
    "        #select the best fitted distribution\n",
    "        sel_dist,p = (max(self.dist_results,key=lambda item:item[1]))\n",
    "        #store the name of the best fit and its p value\n",
    "        self.DistributionName = sel_dist\n",
    "        self.PValue = p\n",
    "\n",
    "        self.isFitted = True\n",
    "        return self.DistributionName, self.PValue\n",
    "    \n",
    "    # for discrete variables\n",
    "    # finds the best fitted distribution\n",
    "\n",
    "    def Fit_D(self, y):\n",
    "        self.dist_results = []\n",
    "        self.params = {}\n",
    "        for dist_name in self.dist_names_d:\n",
    "            dist = getattr(sp, dist_name)\n",
    "            param = dist.fit(y)\n",
    "\n",
    "            self.params[dist_name] = param\n",
    "            #Applying the Kolmogorov-Smirnov test\n",
    "            D, p = sp.stats.kstest(y, dist_name, args=param)\n",
    "            self.dist_results.append((dist_name,p))\n",
    "        #select the best fitted distribution\n",
    "        sel_dist,p = (max(self.dist_results,key=lambda item:item[1]))\n",
    "        #store the name of the best fit and its p value\n",
    "        self.DistributionName = sel_dist\n",
    "        self.PValue = p\n",
    "\n",
    "        self.isFitted = True\n",
    "        return self.DistributionName, self.PValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fit_C() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-7b1e3782bc27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFit_C\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: Fit_C() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "# select a sample and output distribution\n",
    "\n",
    "s = balanced_data.sample(n=50,random_state=4, replace=True)\n",
    "\n",
    "D = Distribution()\n",
    "D.Fit_C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.Fit_C(s['n_duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Check Skew of Feature Distributions\n",
    "\n",
    "#### Notes:\n",
    "\n",
    "- Goodness-of-fit (GOF) tests are not very informative when the sample size is very small or very large.\n",
    "- Log Transformation:\n",
    "- Cube Root Transformation: \n",
    "\n",
    "Source: https://blogs.sas.com/content/iml/2016/11/28/goodness-of-fit-large-small-samples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['n_age'].plot.hist(bins=12, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing purposes - transform distribution of feature\n",
    "\n",
    "s = balanced_data.sample(n=500,random_state=4, replace=True)\n",
    "\n",
    "d = s['n_age']**(1/3)\n",
    "d.plot.hist(bins=12, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_desc = balanced_data.describe()\n",
    "\n",
    "csv_desc['skew'] = csv_desc.skew(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_distribution(ss['n_age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Optimise Curve-Fitting of Distribution to Feature\n",
    "\n",
    "### Statistical Tests\n",
    "\n",
    "Source: https://towardsdatascience.com/data-science-simplified-hypothesis-testing-56e180ef2f71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Replace Missing / NaN with Mean\n",
    "\n",
    "#### Note: \n",
    "\n",
    "The Mean is not a robust measure of central tendency due to the possibility of influence by Outliers.\n",
    "\n",
    "#### Alternatives:\n",
    "\n",
    "- Median\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Robust_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.apply(lambda x: x.fillna(x.mean()),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Scaling Feature\n",
    "\n",
    "Scaling: Transforming your data so that it fits within a specific scale, like 0–100 or 0–1.\n",
    "\n",
    "- StandardScaler: Removes the mean and scales the data to unit variance. \n",
    "- MinMaxScaler: Rescales the data set such that all feature values are in the range [0, 1].\n",
    "- MaxAbsScaler: Differs from the previous scaler such that the absolute values are mapped in the range [0, 1].\n",
    "- RobustScaler: Unlike the previous scalers, the centering and scaling statistics of this scaler are based on percentiles and are therefore not influenced by a few number of very large marginal outlier.\n",
    "\n",
    "Source: https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "\n",
    "distributions = [\n",
    "    ('Unscaled data', X),\n",
    "    ('Data after standard scaling',\n",
    "        StandardScaler().fit_transform(X)),\n",
    "    ('Data after min-max scaling',\n",
    "        MinMaxScaler().fit_transform(X)),\n",
    "    ('Data after max-abs scaling',\n",
    "        MaxAbsScaler().fit_transform(X)),\n",
    "    ('Data after robust scaling',\n",
    "        RobustScaler(quantile_range=(25, 75)).fit_transform(X)),\n",
    "    ('Data after power transformation (Yeo-Johnson)',\n",
    "     PowerTransformer(method='yeo-johnson').fit_transform(X)),\n",
    "    ('Data after power transformation (Box-Cox)',\n",
    "     PowerTransformer(method='box-cox').fit_transform(X)),\n",
    "    ('Data after quantile transformation (gaussian pdf)',\n",
    "        QuantileTransformer(output_distribution='normal')\n",
    "        .fit_transform(X)),\n",
    "    ('Data after quantile transformation (uniform pdf)',\n",
    "        QuantileTransformer(output_distribution='uniform')\n",
    "        .fit_transform(X)),\n",
    "    ('Data after sample-wise L2 normalizing',\n",
    "        Normalizer().fit_transform(X)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an scaler object\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "Scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Transforming Feature Distribution\n",
    "\n",
    "Normalization: Change your observations so that they can be described as a normal distribution. In general, you’ll only want to normalize your data if you’re going to be using a machine learning or statistics technique that assumes your data is normally distributed.\n",
    "\n",
    "- PowerTransformer: Applies a power transformation to each feature to make the data more Gaussian-like. Currently, PowerTransformer implements the Yeo-Johnson and Box-Cox transforms.\n",
    "- QuantileTransformer (Gaussian Output):\n",
    "- QuantileTransformer (Uniform Output):\n",
    "- Normalizer: \n",
    "\n",
    "\n",
    "Refer to the type of distribution.\n",
    "\n",
    "- QuantileTransformer provides non-linear transformations in which distances between marginal outliers and inliers are shrunk. \n",
    "\n",
    "- PowerTransformer provides non-linear transformations in which data is mapped to a normal distribution to stabilize variance and minimize skewness.\n",
    "\n",
    "- On “small” datasets (less than a few hundred points), the quantile transformer is prone to overfitting. The use of the power transform is then recommended. \n",
    "\n",
    "Source: https://medium.com/@nsethi610/data-cleaning-scale-and-normalize-data-4a7c781dd628\n",
    "\n",
    "Source: https://scikit-learn.org/stable/auto_examples/preprocessing/plot_map_data_to_normal.html\n",
    "\n",
    "Source: https://towardsdatascience.com/transforming-skewed-data-73da4c2d0d16\n",
    "\n",
    "Source: https://medium.com/@ODSC/transforming-skewed-data-for-machine-learning-90e6cc364b0\n",
    "\n",
    "Source: https://datamadness.github.io/Skewness_Auto_Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering\n",
    "\n",
    "\n",
    "- Feature Encoding (One Hot Encoding, Label Encoding): \n",
    "- Deep Feature Synthesis: \n",
    "- Memory Reduction: \n",
    "- Remove Outliers: \n",
    "- Replace / Delete NAN: \n",
    "- Combining / Splitting: \n",
    "- Normalize / Standardize: \n",
    "- Isolation Forest: \n",
    "\n",
    "Source: https://towardsdatascience.com/feature-engineering-techniques-in-python-97977ecaf6c8\n",
    "\n",
    "Source: https://www.kdnuggets.com/2019/08/4-tips-advanced-feature-engineering-preprocessing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Reduction: Principal Component Analysis (PAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_PCA(x, n):\n",
    "    '''\n",
    "        Reduce the dimensions using Principal Component Analysis\n",
    "    '''\n",
    "    # create the PCA object\n",
    "    # reduce dimension to the first 'n' variables by estimating a PCA model with dc (sklearn)\n",
    "    \n",
    "    pca = dc.PCA(n_components=n, whiten=True)\n",
    "\n",
    "    # learn the principal components from all the features\n",
    "    # fit a PCA model by using input 'x' array\n",
    "    # return a pca.fit() object\n",
    "    \n",
    "    return pca.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = reduce_PCA(balanced_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = p.transform(balanced_data)\n",
    "pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.\n",
    "\n",
    "Irrelevant or partially relevant features can negatively impact model performance.\n",
    "\n",
    "<b> Benefits of Feature Selection </b>\n",
    "\n",
    "- Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "- Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "- Reduces Training Time: Less data means that algorithms train faster.\n",
    "\n",
    "<b> Techniques </b>\n",
    "\n",
    "- Univariate Selection: Selects features that have the strongest relationship with the output variable.\n",
    "- Recursive Feature Elimination: \n",
    "- Principal Component Analysis: \n",
    "- Feature Importance: \n",
    "\n",
    "Source: https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "\n",
    "Source: https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 8\n",
      "Selected Features: [False  True False  True  True False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False  True  True False  True False False False False False]\n",
      "Feature Ranking: [ 4  1  6  1  1 19 12  5 14  3 18 21 33 28 34 11 22 37  1 38 24 35 23 31\n",
      " 13 17  2 39 27 26 15 25  1  9 36 10  8  1  1 30  1 29 20 32 16  7]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction using Univariate Selection\n",
    "# select a machine learning model to create an object. In this example logistic regression is chosen.\n",
    "\n",
    "model = lr(solver='lbfgs')\n",
    "rfe = RFE(model, 8)\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple usage of Pipeline \n",
    "# that runs successively a univariate feature selection with anova and then a SVM of the selected features.\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_features=20, n_informative=3, n_redundant=0, n_classes=4,\n",
    "    n_clusters_per_class=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# ANOVA SVM-C\n",
    "# 1) anova filter, take 3 best ranked features\n",
    "anova_filter = SelectKBest(f_regression, k=3)\n",
    "# 2) svm\n",
    "clf = svm.LinearSVC()\n",
    "\n",
    "anova_svm = make_pipeline(anova_filter, clf)\n",
    "anova_svm.fit(X_train, y_train)\n",
    "y_pred = anova_svm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "coef = anova_svm[:-1].inverse_transform(anova_svm['linearsvc'].coef_)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_previous</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_euribor3m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job_retired</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default_unknown</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact_telephone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_ctc_outcome_success</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_mar</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_may</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_oct</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [n_duration, n_previous, n_euribor3m, job_retired, default_unknown, contact_telephone, prev_ctc_outcome_success, month_mar, month_may, month_oct]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit selector\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Get columns to keep\n",
    "cols = selector.get_support(indices=True)\n",
    "\n",
    "# Create new dataframe with only desired columns, or overwrite existing\n",
    "features_df_new = X.iloc[:,cols]\n",
    "features_df_new.head(0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Specs       Score\n",
      "32  prev_ctc_outcome_success  675.033761\n",
      "4                n_euribor3m  637.892378\n",
      "31         contact_telephone  423.845057\n",
      "26           default_unknown  282.587124\n",
      "38                 month_may  210.926902\n",
      "40                 month_oct  209.162464\n",
      "37                 month_mar  206.131305\n",
      "2                 n_previous  141.025529\n",
      "1                 n_duration  131.394546\n",
      "9                job_retired  116.582901\n"
     ]
    }
   ],
   "source": [
    "# apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "# concat two dataframes for better visualization\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  # naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  # print 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.84291221e-02 2.58558675e-01 3.23467829e-02 3.89944591e-02\n",
      " 1.40063251e-01 1.10818557e-02 7.10885080e-03 3.69581685e-03\n",
      " 9.76045730e-03 8.43426508e-03 6.07526755e-03 8.98055376e-03\n",
      " 6.37089129e-03 1.43641208e-02 4.16316684e-03 2.60164971e-03\n",
      " 1.55694576e-02 1.39074679e-02 4.93850669e-04 5.37552358e-03\n",
      " 1.11130117e-02 1.40708907e-02 2.08403737e-04 1.01521842e-02\n",
      " 1.19124694e-02 7.41320986e-03 2.49809590e-02 0.00000000e+00\n",
      " 2.47023908e-02 3.88843370e-03 1.77587911e-02 2.31778675e-02\n",
      " 4.17205912e-02 6.16253899e-03 2.53799035e-03 7.39671127e-03\n",
      " 5.52110293e-03 1.09963822e-02 3.46468572e-02 1.06208134e-02\n",
      " 1.30421588e-02 2.76854695e-03 1.63759706e-02 1.47991055e-02\n",
      " 1.37057839e-02 1.39513492e-02]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction using Feature Importance\n",
    "# select a machine learning model to create an object. In this example logistic regression is chosen.\n",
    "\n",
    "model = ExtraTreesClassifier(n_estimators=10)\n",
    "model.fit(X, y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Build RF classifier to use in feature selection\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "# Build step forward feature selection\n",
    "sfs1 = sfs(clf,\n",
    "           k_features=5,\n",
    "           forward=True,\n",
    "           floating=False,\n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "           cv=5)\n",
    "\n",
    "# Perform SFFS\n",
    "sfs1 = sfs1.fit(X_train, y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Which features to select\n",
    "\n",
    "feat_cols = list(sfs1.k_feature_idx_)\n",
    "\n",
    "print(feat_cols)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> Part 2 </u>\n",
    "\n",
    "### Notes:\n",
    "\n",
    "#### Shortcomings of Different Machine Learning Models and How to Resolve Them\n",
    "\n",
    "- Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators.\n",
    "\n",
    "- Metric-based and gradient-based estimators often assume approximately standardized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression\n",
    "# replace with any other python model object\n",
    "\n",
    "lr().get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original dataset and split into train and test datasets\n",
    "\n",
    "train_x, train_y, test_x, test_y, labels = hlp.split_data(data, y='credit_application')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, labels = hlp.split_data(balanced_data, y='credit_application')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Building: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularision: Ridge (R1), Lasso (R2) and Elastic Net\n",
    "\n",
    "To produce a more accurate model of complex data we can add a penalty term to the OLS equation. A penalty adds a bias towards certain values.\n",
    "\n",
    "Source: https://www.knime.com/blog/regularization-for-logistic-regression-l1-l2-gauss-or-laplace?utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Nonlinearity\n",
    "\n",
    "<b> Feature Cross </b>: Synthetic frature that encodes nonlinearity in the feature space by multuplying two or more input features together.\n",
    "\n",
    "Source: https://developers.google.com/machine-learning/crash-course/feature-crosses/encoding-nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian View of Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new method and 'decorate' it with the .timeit() method in helper.py\n",
    "\n",
    "@hlp.timeit\n",
    "def fitNaiveBayes(x,y):\n",
    "    \n",
    "    '''\n",
    "        Build the Naive Bayes classifier\n",
    "    '''\n",
    "    \n",
    "    # create the classifier object; we use Gaussian NBs.\n",
    "    naiveBayes_classifier = nb.GaussianNB()\n",
    "\n",
    "    # fit the model; every classifier object has a .fit() method.\n",
    "    return naiveBayes_classifier.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a testing set by calling the .split_data() method\n",
    "# in helper.py. The default text size is 1/3 of the data; you can change this, too.\n",
    "# Mind the output of the method including train_x, train_y, test_x, test_y, and labels\n",
    "\n",
    "# train the model by using train_x and train_y sets.\n",
    "classifier = fitNaiveBayes(train_x, train_y)\n",
    "\n",
    "# classify the test data by using the test_x dataset, and predict the results.\n",
    "predicted = classifier.predict(test_x)\n",
    "\n",
    "# print out the results by calling the printModelSummary()\n",
    "\n",
    "hlp.printModelSummary(test_y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "#### Probability and Likelihood\n",
    "\n",
    "The distinction between probability and likelihood is fundamentally important: Probability attaches to possible results; likelihood attaches to hypotheses.\n",
    "\n",
    "#### Loss / Cost Function\n",
    "\n",
    "Maximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters.\n",
    "\n",
    "- MLE is consistent with Ordinary Least Square (OLS).\n",
    "- With infinite data, it will estimate the optimal β, and approximate it well for small but robust datasets.\n",
    "- MLE is efficient; no consistent estimator has lower asymptotic mean squared error than MLE.\n",
    "\n",
    "Then why use MLE for Logistic Regression optimisation instead of OLS\n",
    "\n",
    "- MLE is generalizable for regression and classification.\n",
    "- MLE is efficient; no consistent estimator has lower asymptotic error than MLE if you’re using the right distribution.\n",
    "\n",
    "Source: https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hlp.timeit\n",
    "\n",
    "def LogitClassifier(X):\n",
    "    \n",
    "    '''\n",
    "        Build a Logistic Regression classifier\n",
    "    '''\n",
    "    \n",
    "    train_X, train_y, test_X, test_y, labels = hlp.split_data(X, y = 'credit_application')\n",
    "    \n",
    "    L = lr()\n",
    "    LF = L.fit(train_X, train_y)\n",
    "    predicted = LF.predict(test_X)\n",
    "    summary = hlp.printModelSummary(test_y, predicted)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run original data without preprocessing\n",
    "\n",
    "LogitClassifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run data after preprocessing at achieve balance in the target classes\n",
    "\n",
    "LogitClassifier(balanced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fitDecisionTree() method and 'decorate' with @hlp.timeit, so we can time\n",
    "# how long it takes to run the model fitting process\n",
    "\n",
    "@hlp.timeit\n",
    "def fitDecisionTree(x,y):\n",
    "    \n",
    "    '''\n",
    "        Build a decision tree classifier\n",
    "    '''\n",
    "    \n",
    "    # create the classifier object\n",
    "    # we specify that any decision node cannot hold less than 1,000 data points\n",
    "    # because we have quite some data\n",
    "    \n",
    "    decision_tree = tree.DecisionTreeClassifier(min_samples_split=1000)\n",
    "\n",
    "    # fit the data\n",
    "    return decision_tree.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the DT model\n",
    "classifier = fitDecisionTree(train_x, train_y)\n",
    "\n",
    "# classify the test data\n",
    "predicted = classifier.predict(test_x)\n",
    "\n",
    "# print out the results\n",
    "hlp.printModelSummary(test_y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Bagging classifier\n",
    "\n",
    "# Example one\n",
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                 sampling_strategy='auto',\n",
    "                                 replacement=False,\n",
    "                                 random_state=0)\n",
    "bbc.fit(X_train, y_train) \n",
    "y_pred = bbc.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "#### Introduction to Ensemble Learning\n",
    "\n",
    "Basic Ensemble Techniques\n",
    "- Max Voting\n",
    "- Averaging\n",
    "- Weighted Average\n",
    "\n",
    "Advanced Ensemble Techniques\n",
    "- Stacking\n",
    "- Blending\n",
    "- Bagging\n",
    "- Boosting\n",
    "\n",
    "Algorithms based on Bagging and Boosting\n",
    "- Bagging meta-estimator\n",
    "- Random Forest\n",
    "- AdaBoost\n",
    "- GBM\n",
    "- XGB\n",
    "- Light GBM\n",
    "- CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've already imported the following libs:\n",
    "# import helper as hlp\n",
    "# import pandas as pd\n",
    "# import sklearn.tree as sk\n",
    "\n",
    "import sklearn.ensemble as en\n",
    "\n",
    "\n",
    "#define a fitRandomForest() method so we can throw our data into it, time its execution and run the model.\n",
    "\n",
    "@hlp.timeit\n",
    "def fitRandomForest(x,y):\n",
    "    \n",
    "    '''\n",
    "        Build a random forest classifier\n",
    "    '''\n",
    "    \n",
    "    # create the classifier object; minimum 100 samples at each node to perform a split.\n",
    "    # default to form 10 trees in the forest; can be adjusted as wish.\n",
    "    \n",
    "    forest = en.RandomForestClassifier(min_samples_split=100, n_estimators=10)\n",
    "\n",
    "    # fit the data\n",
    "    \n",
    "    return forest.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets as usual.\n",
    "# we still predict on the same Y (credit_application) and use only a sub-set of the whole X.\n",
    "\n",
    "trainForest_x, trainForest_y,testForest_x,  testForest_y, forest_labels = hlp.split_data(\n",
    "    data, \n",
    "    y = 'credit_application',\n",
    "    x = ['n_duration','n_nr_employed',\n",
    "        'prev_ctc_outcome_success','n_euribor3m',\n",
    "        'n_cons_conf_idx','n_age','month_oct',\n",
    "        'n_cons_price_idx','edu_university_degree','n_pdays',\n",
    "        'dow_mon','job_student','job_technician',\n",
    "        'job_housemaid','edu_basic_6y']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Random Forest Classifer\n",
    "forest_classifier = fitRandomForest(trainForest_x, trainForest_y)\n",
    "\n",
    "# classify the testing data set\n",
    "forest_predicted = forest_classifier.predict(testForest_x)\n",
    "\n",
    "# print out the results\n",
    "hlp.printModelSummary(testForest_y, forest_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Model Building: Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the findClusters_Kmeans() method, so we can call it in our Python script\n",
    "\n",
    "@hlp.timeit\n",
    "def findClusters_kmeans(data):\n",
    "    \n",
    "    '''\n",
    "        Cluster data using K-means\n",
    "    '''\n",
    "    \n",
    "    # create the Kmenas classifier object by calling cl.KMeans(); \n",
    "    # by default, we estimate 4 clusters.\n",
    "    # The n_init parameter controls how many models to estimate by KMeans().\n",
    "    \n",
    "    kmeans = c1.KMeans(n_clusters=4,n_jobs=-1,verbose=0,n_init=30)\n",
    "\n",
    "    # fit the data\n",
    "    return kmeans.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables (columns) that we are interested in to cluster\n",
    "\n",
    "selectedVars = data[\n",
    "        ['n_duration',\n",
    "         'n_nr_employed',\n",
    "        'prev_ctc_outcome_success',\n",
    "         'n_euribor3m',\n",
    "        'n_cons_conf_idx',\n",
    "         'n_age','month_oct',\n",
    "        'n_cons_price_idx',\n",
    "         'edu_university_degree',\n",
    "         'n_pdays',\n",
    "        'dow_mon',\n",
    "         'job_student',\n",
    "         'job_technician',\n",
    "        'job_housemaid',\n",
    "         'edu_basic_6y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster the data by calling the method findClusters_kmeans()\n",
    "\n",
    "cluster = findClusters_kmeans(selectedVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the clusters effectiveness\n",
    "\n",
    "labels = cluster.labels_\n",
    "centroids = cluster.cluster_centers_\n",
    "\n",
    "# we call the printClustersSummary() from the helper.py script to print out performance metrics for us\n",
    "# check the text for more details on these metrics and how there are calcualted.\n",
    "\n",
    "hlp.printClustersSummary(selectedVars, labels, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Hyperparameter Tuning and Optimisation\n",
    "\n",
    "Source: https://www.imperial.ac.uk/media/imperial-college/faculty-of-engineering/computing/public/1819-ug-projects/MatacheC-Efficient-Design-of-Machine-Learning-Hyperparameter-Optimizers.pdf\n",
    "\n",
    "Source: https://towardsdatascience.com/algorithms-for-hyperparameter-optimisation-in-python-edda4bdb167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hlp.timeit\n",
    "def Decision_Tree_parameters():\n",
    "    \n",
    "    '''\n",
    "    dataset = datasets.make_classification(n_samples=100, n_features=20, n_informative=5,\n",
    "                n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2,\n",
    "                weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0,\n",
    "                scale=1.0, shuffle=True, random_state=None)\n",
    "    X = dataset[0]\n",
    "    y = dataset[1]\n",
    "    print(y)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Create an scaler object\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    # Create a pca object\n",
    "    pca = decomposition.PCA()\n",
    "\n",
    "    # Create a logistic regression object with an L2 penalty\n",
    "    dtreeClf = tree.DecisionTreeClassifier()\n",
    "\n",
    "    # Create a pipeline of three steps. First, standardize the data.\n",
    "    # Second, tranform the data with PCA.\n",
    "    # Third, train a Decision Tree Classifier on the data.\n",
    "    pipe = Pipeline(steps=[('sc', sc),\n",
    "                           ('pca', pca),\n",
    "                           ('dtreeClf', dtreeClf)])\n",
    "\n",
    "    # Create Parameter Space\n",
    "    # Create a list of a sequence of integers from 1 to 30 (the number of features in X + 1)\n",
    "    n_components = list(range(1,X.shape[1]+1,1))\n",
    "\n",
    "    # Create lists of parameter for Decision Tree Classifier\n",
    "    \n",
    "    criterion = ['gini', 'entropy']\n",
    "    max_depth = [4,6,8,12]\n",
    "\n",
    "    # Create a dictionary of all the parameter options \n",
    "    # Note has you can access the parameters of steps of a pipeline by using '__’\n",
    "    parameters = dict(pca__n_components=n_components,\n",
    "                      decisiontree__criterion=criterion,\n",
    "                      decisiontree__max_depth=max_depth)\n",
    "\n",
    "    # Conduct Parameter Optmization With Pipeline\n",
    "    # Create a grid search object\n",
    "    clf = GridSearchCV(pipe, parameters)\n",
    "\n",
    "    # Fit the grid search\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # View The Best Parameters\n",
    "    print('Best Criterion:', clf.best_estimator_.get_params()['decisiontree__criterion'])\n",
    "    print('Best max_depth:', clf.best_estimator_.get_params()['decisiontree__max_depth'])\n",
    "    print('Best Number Of Components:', clf.best_estimator_.get_params()['pca__n_components'])\n",
    "    print(); print(clf.best_estimator_.get_params()['decisiontree'])\n",
    "\n",
    "    # Use Cross Validation To Evaluate Model\n",
    "    CV_Result = cross_val_score(clf, X, y, cv=4, n_jobs=-1)\n",
    "    print(); print(CV_Result)\n",
    "    print(); print(CV_Result.mean())\n",
    "    print(); print(CV_Result.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision_Tree_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hlp.timeit\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 500, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(3, 10, num = 1)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(features_train, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Logistic Regression object \n",
    "\n",
    "clf = lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually set the hyperparameters\n",
    "# for the grid_search_wrapper function\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "    'fit_intercept': [False,True],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that will run grid search to find the parameters\n",
    "# that will optimise the machine learning object\n",
    "# in this case the focus is on the logistic regression model\n",
    "\n",
    "def grid_search_wrapper(refit_score='recall_score'):\n",
    "    \n",
    "    \"\"\"\n",
    "        fits a GridSearchCV classifier using refit_score for optimization\n",
    "        prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # scores = cross_val_score(clf, X, y, cv=5)\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n",
    "                           cv=skf, return_train_score=True)\n",
    "    grid_search.fit(train_x.values, train_y.values)\n",
    "\n",
    "    # make the predictions\n",
    "    pred_y = grid_search.predict(test_x.values)\n",
    "\n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # confusion matrix on the test data.\n",
    "    print('\\nConfusion matrix of Logistic regression optimised for {} on the test data:'.format(refit_score))\n",
    "    summary = pd.DataFrame(confusion_matrix(test_y, pred_y),\n",
    "                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos'])\n",
    "    \n",
    "    summary_2 = hlp.printModelSummary(test_y, pred_y)\n",
    "    \n",
    "    return summary, summary_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr. Richard Wu's Logistic Regression Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the parameters of the python logistic regression object\n",
    "\n",
    "lr().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='credit_application', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess the dataset to balance the classes in the target variable\n",
    "# and take a larger sample size\n",
    "\n",
    "# shuffle the dataset and select n observations\n",
    "shuffled_df = data.sample(n=40000,random_state=4, replace=True)\n",
    "\n",
    "# Put all the credit_application in a separate dataset.\n",
    "c_app_df = shuffled_df.loc[shuffled_df['credit_application'] == 1]\n",
    "\n",
    "# Randomly select n observations from the non-fraud (majority class)\n",
    "c_non_app_df = shuffled_df.loc[shuffled_df['credit_application'] == 0].sample(n=4400,random_state=42, replace=True)\n",
    "\n",
    "# Concatenate both dataframes again\n",
    "new_balanced_data = pd.concat([c_app_df, c_non_app_df])\n",
    "\n",
    "# plot the dataset after the undersampling\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot('credit_application', data=new_balanced_data)\n",
    "plt.title('Balanced Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8929, 60)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_balanced_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_balanced_data_2 = shuffle(new_balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 8\n",
      "Selected Features: [False  True False  True  True False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False  True  True False  True False False False False False]\n",
      "Feature Ranking: [ 4  1  6  1  1 19 12  5 14  3 18 21 33 28 34 11 22 37  1 38 24 35 23 31\n",
      " 13 17  2 39 27 26 15 25  1  9 36 10  8  1  1 30  1 29 20 32 16  7]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction using Univariate Selection\n",
    "# select a machine learning model to create an object. In this example logistic regression is chosen.\n",
    "\n",
    "model = lr(solver='lbfgs')\n",
    "rfe = RFE(model, 8)\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_previous</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_euribor3m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job_retired</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default_unknown</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact_telephone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_ctc_outcome_success</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_mar</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_may</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_oct</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [n_duration, n_previous, n_euribor3m, job_retired, default_unknown, contact_telephone, prev_ctc_outcome_success, month_mar, month_may, month_oct]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df_new.head(0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = new_balanced_data[['n_duration', \n",
    "                       'n_pdays', \n",
    "                       'n_previous', \n",
    "                       'n_emp_var_rate', \n",
    "                       'n_euribor3m', \n",
    "                       'n_nr_employed',\n",
    "                       'contact_cellular',\n",
    "                       'contact_telephone',\n",
    "                       'prev_ctc_outcome_nonexistent',\n",
    "                       'prev_ctc_outcome_success',\n",
    "                       'credit_application']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, labels = hlp.split_data(d, y='credit_application')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 predictors and target after removing collinear features and scoring predictors\n",
    "\n",
    "dd = new_balanced_data_2[['n_duration',\n",
    "                        'n_previous',\n",
    "                        'n_euribor3m',\n",
    "                        'job_retired',\n",
    "                        'default_unknown',\n",
    "                        'contact_telephone',\n",
    "                        'prev_ctc_outcome_success',\n",
    "                        'month_mar',\n",
    "                        'month_may',\n",
    "                        'month_oct',\n",
    "                        'credit_application']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, labels = hlp.split_data(dd, y='credit_application')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy of the model is 87.16 percent\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86      1429\n",
      "         1.0       0.85      0.91      0.88      1499\n",
      "\n",
      "    accuracy                           0.87      2928\n",
      "   macro avg       0.87      0.87      0.87      2928\n",
      "weighted avg       0.87      0.87      0.87      2928\n",
      "\n",
      "Confusion matrix: \n",
      " [[1191  238]\n",
      " [ 138 1361]]\n",
      "ROC:  0.8706942953804986\n"
     ]
    }
   ],
   "source": [
    "# input parameter values for the logistic regression object \n",
    "# base on values obtained from hyperparameter tuning\n",
    "L = lr(C=5.0, fit_intercept=False, penalty='l1')\n",
    "\n",
    "# train the model\n",
    "LF = L.fit(train_x, train_y)\n",
    "\n",
    "# output \n",
    "predicted = LF.predict(test_x)\n",
    "\n",
    "# output summary evaluation of metrics\n",
    "summary = hlp.printModelSummary(test_y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimise recall\n",
    "\n",
    "grid_search_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dr. Richard Wu's Logistic Regression Homework: Removed Collinear Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy of the model is 87.16 percent\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86      1429\n",
      "         1.0       0.85      0.91      0.88      1499\n",
      "\n",
      "    accuracy                           0.87      2928\n",
      "   macro avg       0.87      0.87      0.87      2928\n",
      "weighted avg       0.87      0.87      0.87      2928\n",
      "\n",
      "Confusion matrix: \n",
      " [[1191  238]\n",
      " [ 138 1361]]\n",
      "ROC:  0.8706942953804986\n"
     ]
    }
   ],
   "source": [
    "# REMOVED COLLINEAR FEATURES\n",
    "\n",
    "# input parameter values for the logistic regression object \n",
    "# base on values obtained from hyperparameter tuning\n",
    "L = lr(C=5.0, fit_intercept=False, penalty='l1')\n",
    "\n",
    "# train the model\n",
    "LF = L.fit(train_x, train_y)\n",
    "\n",
    "# output \n",
    "predicted = LF.predict(test_x)\n",
    "\n",
    "# output summary evaluation of metrics\n",
    "summary = hlp.printModelSummary(test_y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for recall_score\n",
      "{'C': 5.0, 'fit_intercept': False, 'penalty': 'l1'}\n",
      "\n",
      "Confusion matrix of Logistic regression optimised for recall_score on the test data:\n",
      "Overall accuracy of the model is 87.16 percent\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86      1429\n",
      "         1.0       0.85      0.91      0.88      1499\n",
      "\n",
      "    accuracy                           0.87      2928\n",
      "   macro avg       0.87      0.87      0.87      2928\n",
      "weighted avg       0.87      0.87      0.87      2928\n",
      "\n",
      "Confusion matrix: \n",
      " [[1191  238]\n",
      " [ 138 1361]]\n",
      "ROC:  0.8706942953804986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(     pred_neg  pred_pos\n",
       " neg      1191       238\n",
       " pos       138      1361, None)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  dd.drop(['credit_application'], axis=1)\n",
    "target = dd['credit_application']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87306843 0.89514349 0.86313466 0.88300221 0.87513812]\n"
     ]
    }
   ],
   "source": [
    "L = lr(C=5.0, fit_intercept=False, penalty='l1')\n",
    "\n",
    "scores = cross_val_score(clf, features, target, cv=5, scoring='recall')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8929, 11)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Model Evaluation\n",
    "\n",
    "Metrics for evaluating predicted class labels:\n",
    "\n",
    "- Accuracy\n",
    "- Recall\n",
    "- Precision\n",
    "- Specificity\n",
    "- Sensitivity\n",
    "- G-Mean\n",
    "- F1-Measure\n",
    "- F0.5-Measure\n",
    "- F2-Measure\n",
    "\n",
    "Metrics for evaluating predicted probabilities:\n",
    "\n",
    "- ROC Area Under Curve (ROC AUC).\n",
    "- Precision Recall Area Under Curve (PR AUC).\n",
    "- Brier Score\n",
    "- Matthews correlation coefficient (MCC)\n",
    "\n",
    "<b> AUC </b>: \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).\n",
    "\n",
    "Source: https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation\n",
    "\n",
    "Source: https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n",
    "\n",
    "Source: https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c\n",
    "\n",
    "Source: https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(label_train,pd.Series(pred_train),rownames=['ACTUAL'],colnames=['PRED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC/AUC\n",
    "\n",
    "- Receiver Operating Characteristic (ROC): Probability curve\n",
    "\n",
    "- Area Under the Curve (AUC): Represents degree or measure of separability. The higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. When AUC is 0.7 for example, it means there is 70% chance that model will be able to distinguish between positive class and negative class.\n",
    "\n",
    "Source: https://towardsdatascience.com/end-to-end-python-framework-for-predictive-modeling-b8052bb96a78\n",
    "\n",
    "Source: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n",
    "\n",
    "Source: https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.charts import Histogram\n",
    "from ipywidgets import interact\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "output_notebook()\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict_proba(features_train)[:,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(np.array(label_train), preds)\n",
    "\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "\n",
    "p = figure(title=\"ROC Curve - Train data\")\n",
    "r = p.line(fpr,tpr,color='#0077bc',legend = 'AUC = '+ str(round(auc,3)), line_width=2)\n",
    "s = p.line([0,1],[0,1], color= '#d15555',line_dash='dotdash',line_width=2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decile Plots and Kolmogorov Smirnov (KS) Statistic\n",
    "\n",
    "Source: https://towardsdatascience.com/end-to-end-python-framework-for-predictive-modeling-b8052bb96a78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes: Target Leakage\n",
    "\n",
    "Target leakage occurs when a variable that is not a feature is used to predict the target.\n",
    "\n",
    "For example, if your training dataset that was normalized or standardized using missing value imputation (such as min, max, mean) - such that the training dataset had full knowledge of the distribution of data in the training dataset. An unseen dataset would not have any knowledge of the distribution of data in the dataset - that is, a row in the unseen dataset would not know if there were three records or three million other records. It wouldn't be able to normalize or standardize itself using unseen data. As a result, the model ends up overfitting to the training data and produces a higher accuracy when run on unseen data than it would if the model had not fit the training dataset so well (too well).\n",
    "\n",
    "Source: http://downloads.alteryx.com/betawh_xnext/MachineLearning/MLTargetLeakage.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
